{"cells":[{"cell_type":"markdown","source":["# **Part 0: Introduction to Yelp Dataset Project**"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Part 0.0: Yelp Academic Dataset\n\nThis project delves into exploratory analysis and building predictive models using the [Yelp academic dataset](https://www.yelp.com/dataset_challenge/). It is an opportunity for you to explore a machine learning problem in the context of a real-world data set using big data analysis tools. In order to use the dataset and finish this project, you must agree to the dataset's terms of use provided [here](https://www.yelp.com/html/pdf/Dataset_Challenge_Academic_Dataset_Agreement.pdf).\n\nWe have chosen a subset of the Yelp academic dataset for you to work with. This subsampled data is loaded into RDDs in section (1). The complete dataset is available from Yelp's website [here](https://www.yelp.com/dataset_challenge/dataset). Remember that you are limited by the DataBricks Community Edition's limits on memory and computation. Yelp has provided some example code at their Github repository [here](https://github.com/Yelp/dataset-examples) that might be helpful in getting started. However, these are pure Python code and not Spark code that provide parallelism.\n\nBy design, the project is open-ended; you are free to decide how you want to approach the problem and what tools you want to employ. We want to see a best-effort solution that utilizes what you learned in class and also potentially trying new things beyond class. Your project will be worth 20% of your final class grade."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Part 0.1: Grading Rubric:\n\n** Course staff will use the following rubric when grading your final project reports: **\n\n\n*  *Introduction/Motivation/Problem Definition (10%)*\n  * Identify, define, and motivate the problem that you are addressing.\n  * How (precisely) will a machine learning solution address the problem?\n\n*  *Data Understanding and Preparation (15%)*\n  * What preliminary analyses have you performed on the data? What observations have you made? How did those observations help shape your approach?\n  * Provide the preliminary data analysis results and your observations.\n  * Specify how the data will be transformed to the format required for machine learning. \n\n*  *Methodology (35%)*\n  * This is where you give a detailed description of your primary contributions. It is especially important that this part be clear and well written so that we can fully understand what you did.\n  * Specify the type of model(s) built and/or information/knowledge extracted.\n  * Discuss choices for machine learning algorithm: what are other alternatives, and what are their pros and cons (in the context of the problem and as compared to your proposed solution)?\n  * Discuss why and how this model should \"solve\" the problem (i.e., improve along some dimension of interest). \n  * Outline the big data analysis tools and libraries you have used. \n\nIt is not so important how well your method performs but rather, (a) how thorough and careful your methodology is, and (b) how interesting and clever the approaches you took and the tools you have used are. \n\n*  *Evaluation and Results (30%)*\n  * We are interested in seeing a clear and conclusive set of experiments which successfully evaluate the problem you set out to solve. Make sure to interpret the results and talk about what we can conclude and learn from your approach.\n  * How do you evaluate your machine learning solution to the specific question(s) you have addressed?\n  * What do these results tell you about your solution?\n  * Present and discuss your evaluation results and findings. You may use tables or figures (e.g. ROC plot) to visualize your results.\n\n*  *Style and writing (10%)*\n  * Overall writing, grammar, organization, figures and illustrations.\n \nNote that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.2: Code of Conduct **\n\n** Please follow the following guidelines with respect to collaboration: **\n\n* You have to use the data we have provided you. You cannot choose your own dataset. By using the dataset, you agree to Yelp's terms of use available [here](https://www.yelp.com/html/pdf/Dataset_Challenge_Academic_Dataset_Agreement.pdf).\n* You will be given 48 hours to work on the project. Use of late days are not allowed for this submission.\n* You are free to use the Web, APIs, ML toolkits, etc. in this project to your best benefit. Please credit any online or offline sources (even casual sources like StackOverflow) if you use them in the project.\n* Project is to be done individually. No collaboration is allowed between students. No discussion is allowed about the project with anyone else except the class instructors. Students who use each other's ideas or code will be heavily penalized."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### **Part 0.3: Project Suggestions **\n\nBefore you embark on the project, please plan out your task by breaking it into smaller chunks that incrementally build on top of each other. For example, you may begin with a simpler set of features and then add more complex features to the dataset. Such modular planning will ensure that you will have a working deliverable in case you run out of time tackling more complicated aspects of the project you had planned to complete. Try to have a barebones but working version of the project after 24 hours, and build on it in the next 24 hours leading up to the deadline. Create a backup version of your notebook after finishing a substantial chunk of the work so you can go back to a working version in case of a catastrophe.\n\n**Here is a list of potential aspects you can tackle in this final open-ended project:**\n\n*  *Exploratory Data Analysis (Perform those that help you get started with your chosen business question, not all of these.)*\n  * Plot a map showing the locations of various businesses. Helper code to help in the creation of maps using \"mpl_toolkits\" is provided in a later section on exploratory data analysis.\n  * Plot a map showing the locations of businesses checkins made by Yelp users.\n  * Plot a histogram of ratings that the businesses get i.e. see how many businesses got ratings of 1-5 each. Is this distribution skewed? Are there ratings that are used rarely as compared to others?\n  * What are the most popular keywords that reviewers use by city or state?\n  * What are the most popular keywords that reviewers use for American/Thai/Chinese restaurants by state?\n  * What are the most popular keywords or adjectives that reviewers use for American/Thai/Chinese restaurants by state?\n  * What are the most popular keywords or adjectives that reviewers use in 5 star reviews for American/Thai/Chinese restaurants by state?\n  * What are the most frequent keywords or adjectives that reviewers use in 1 star reviews for American/Thai/Chinese restaurants by state?\n  * Does the distribution of restaurants with parking space or outdoor seating differ from state to state or city to city?\n  * Are there temporal trends (daily, weekly, holidays) associated with business checkins?\n  * Does the number of checkins per restaurant differ across various restaurant categories?\n  * Is there a correlation between how long a user has been \"yelping\" and the number of reviews he has written?\n  * Is there a correlation between how many friends/fans a user has and the number of votes his reviews get?\n  * What are the 5 most common types of restaurants in each city?\n  * What is the fraction of businesses that accounts for restaurants?\n  * Do the typical business hours vary by city and by type of business?\n  * What does the histogram of number of friends/fans of Yelp users look like? Is it long-tailed or does it follow a certain distribution?\n  * What is the distribution of number of reviews by neighborhood?\n  * Is there a correlation between the star rating and length of reviews?\n  * What are the top keywords or adjectives used by the two genders (male and female, sorry for being binary) in their reviews?\n  * What fraction of Yelp users is male? What fraction is female? What if the fraction of users for whom gender cannot be determined based on the list of male and female names provided in this notebook?\n  * What is the average number of friends/fans for male and female users?\n  * What is the average number of reviews written by male and female users?\n*  *Classification (Any classification task should also include a description of all the features used and which of these features impacted classification performance the most and why.)*\n  * Classify businesses into various business categories (restaurant, dry cleaner, auto body, etc.).\n  * Classify businesses by the type of parking they provide (street, garage, valet, etc.).\n  * Predict the location of a reviewer (east or west coast or mid-country).\n  * Predict the location of a business (east or west coast or mid-country).\n  * Predict if a review is funny, cool, or useful (label should be based on the corresponding votes associated with the review, votes therefore may not be used as features).\n  * Predict which type of restaurant a user reviews most based on the restaurant types reviewed by his friends.\n  * Given the current categories of a business, predict a new category that it could be labeled as. You will need businesses - each with mutiple categories - to hold out some categories randomly from each business for testing purposes.\n  * Predict if two users are friends based on the locations of businesses for which they have written reviews and other user characteristics.\n  * Based on businesses reviewed by a user until a certain timepoint, predict the type of business the user might review next.\n  * Predict if the ratings for a business are going to increase or decline with time. Are some types of restaurants more inclines to suffer from declining ratings?\n  * Predict the gender of Yelp user based on businesses they have written reviews for. Examine a few examples of Yelp users where your classifier is incorrect, and provide any insighful suggestions for improving the classifier.\n  * Predict the gender of Yelp users based on their business reviews. Examine your model to determine if and how the two genders use different words when writing reviews.\n  * Predict the gender of Yelp users based on the numbers of various types of votes their reviews get and the numbers of various types of compliments they receive. Examine a few examples of Yelp users where your classifier is incorrect, and provide any insighful suggestions for improving the classifier.\n*  *Regression (Any regression task should also include a description of all the features used and which of these features impacted regression performance the most and why.)*\n  * Predict the average rating of a business from its reviews and other business characteristics such as location.\n  * Predict the total number of reviews on a given week for each business.\n  * Predict the total number of checkins based on business location, type, and other business characteristics present in \"attributes\" such as \"Happy Hour, \"Accepts Credit Cards\", \"Good For Groups\", \"Outdoor Seating\", and \"Price Range\".\n  * Predict the number of compliments received by a user.\n  * Predict the number of friends a user has on Yelp based on user characteristics like number of reviews written by him, compliments received, etc.\n  * Based on reviews written by a user until a certain timepoint, predict the star rating the user will give as part of his next review. Are certain users more likely to give extreme ratings to reviewed businesses than others?\n  * Predict the number of funny/cool/useful votes sent by a user. Does it depend significantly on how long the user has been \"yelping\" or on gender?\n  * Predict the number of funny/cool/useful votes received by a review. Does it depend significantly on how long the user has been \"yelping\" or on gender?\n* *Clustering*\n  * Cluster business by using their features using a clustering algorithm such as [K-Means](https://spark.apache.org/docs/2.1.0/mllib-clustering.html#k-means). Choose the number of clusters in a data-driven fashion such as by using the elbow heuristic. Analyze clusters and see if they are homogeneous i.e. the business within each cluster look similar and as if they belong within the same group.\n  * Cluster users based on their characteristics. See if users in the same cluster patronize similar businesses.\n* *Recommendation Systems*\n  * Given previous ratings by a Yelp user, recommend other businesses that the user might like. See [Collaborative Filtering on Spark](https://spark.apache.org/docs/2.1.0/mllib-collaborative-filtering.html).\n  * You may also think of restaurant/business recommendation as a link prediction problem. You can use GraphX for this task.\n* *Discovering Insights using Unsupervised Algorithms*\n  * In the Yelp user dataset, you are provided a social network in the form of friends of each Yelp user. You may perform social network analysis using GraphX. This can help you discover the most influential users by eigen-centrality. You may use other measures of network centrality besides eigen-centrality.\n  * Discover dense clusters of closely connected friends and see if they patronize the same businesses.\n  * Verify if the dense clusters of closely connected friends are also homogeneous in terms of gender.\n  * Learn a set of topics by applying topic modeling algorithms such as [LDA](https://spark.apache.org/docs/2.1.0/mllib-clustering.html#latent-dirichlet-allocation-lda) on textual reviews of businesses. Choose the number of topics in a data-driven fashion such as by using a figure that plots perplexity versus number of topics. Explore if the topics are insightful and whether or not they can be used as inputs to some predictive algorithms (see Classification tasks above).\n  * Perform the above topic modeling precedure on the reviews of male and female reviewers separately to obtain two topic models. Explore if the topics are insightful and/or useful in any predictive tasks.\n  * Apply PCA to the matrix where rows are businesses and columns are features of the businesses such as parking type, location, etc. Choose the number of components in a data-driven fashion such as by using a scree plot. Explore if the top components are insightful and can be used as inputs to any predictive algorithms (see classification tasks above).\n  \nYour task is to choose one of these ML problems, or define your own, on the provided dataset and address the problem of your choice with the big data analysis tools you learned during the course as well as others you explore based on the APIs in Spark. If you create your own question, please be sure to state it clearly at the beginning of section 4 (methodology)."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.4: Setup your DataBricks CE Spark Cluster and IPython Notebook **\n\nStep I: Visit the web interface of DataBricks Community Edition at https://community.cloud.databricks.com/.\n\nStep II: Start a DataBricks Community Edition cluster by selecting \"New Cluster\" from the homepage.\n\nStep III: Give your cluster a name and click on \"Create Cluster\". This creates a single node cluster with 6GB memory for your account.\n\nStep IV: Go back to homepage, and choose \"Import Notebook.\" Upload the IPython assignment notebook by following the prompts and open the notebook. Rename the notebook from \"project_yelp_dataset.ipynb\" to \"andrewid_project_yelp_dataset.ipynb\" where \"andrewid\" is your actual Andrew ID.\n\nStep V: By default, the notebook is not attached to a Spark cluster and will show \"Detached\" as its status at the top of the notebook in the browser. Click on the \"Detached\" status and attach it to your cluster. It should now show the message as \"Attached (cluster name)\"\n\nStep VI: You can now import pyspark into the notebook. Also, attaching to the DataBricks Community Edition cluster automatically provides the SparkContext variable \"sc\" to the Python code in your notebook. Use it to create RDDs and write further Spark code.\n\nThese instructions are detailed with screenshots in slides 10-16 of the setup recitation available at https://www.andrew.cmu.edu/user/amaurya/docs/95869/hadoop-spark-setup-recitation.pdf"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### ** Part 0.5: Submission Instructions **\n\nYou will submit both a zipped file on Blackboard and a hardcopy to the TA Abhinav Maurya in HBH 3026. If the TA's office is closed, please slide the hardcopy under the door.\n\nPlease complete the project, and feel free to add new cells as required. Upon completion, execute all cells in the completed notebook, and make sure all results show up. Export the contents of the notebook by choosing \"File > Export > HTML\" and saving the resulting file as \"andrewid_project_yelp_dataset.html\" Place the two files \"andrewid_project_yelp_dataset.ipynb\" and \"andrewid_project_yelp_dataset.html\" in a folder, zip the folder to a zipped file named \"andrewid_project.zip\" and submit it to Blackboard by the deadline. In addition, print the HTML file and submit the hardcopy to the TA Abhinav Maurya in HBH 3026 by the deadline. If the TA's office is closed, please slide the hardcopy under the door."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["# ** Part 1: Load the datasets required for the project **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["We will load four datasets for this project. Please feel free to reasonably subsample the dataset depending on the question you are answering and its complexity. If you choose to subsample any of the four datasets, please explain why you subsampled it and what was the number of datapoints you were left with in the subsampled version. In addition to the four datasets, we will also load two lists which contain names by gender. These lists are helpful in assigning a gender to a Yelp user by their name, since gender is not available in the Yelp dataset."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["import json\nimport os\nimport sys\nimport os.path\nimport pyspark\nimport urllib2\nimport numpy as np\n\n# helper function to load a JSON dataset from a publicly accessible url\ndef get_rdd_from_url(url):\n  response = urllib2.urlopen(url)\n  str_contents = response.read().strip().split('\\n')\n  json_contents = [json.loads(x) for x in str_contents]\n  rdd = sc.parallelize(json_contents)\n  return rdd"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["The first dataset we are going to load is information about Yelp businesses. The information of each business will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n* \"business_id\":\"encrypted business id\"\n* \"name\":\"business name\"\n* \"neighborhood\":\"hood name\"\n* \"address\":\"full address\"\n* \"city\":\"city\"\n* \"state\":\"state -- if applicable --\"\n* \"postal code\":\"postal code\"\n* \"latitude\":latitude\n* \"longitude\":longitude\n* \"stars\":star rating, rounded to half-stars\n* \"review_count\":number of reviews\n* \"is_open\":0/1 (closed/open)\n* \"attributes\":[\"an array of strings: each array element is an attribute\"]\n* \"categories\":[\"an array of strings of business categories\"]\n* \"hours\":[\"an array of strings of business hours\"]\n* \"type\": \"business\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about Yelp businesses in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nbusinesses_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_business.json')\nbusinesses_rdd.cache()\nprint businesses_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The second dataset we are going to load is information about Yelp users. Each user's information will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"user_id\":\"encrypted user id\"\n*  \"name\":\"first name\"\n*  \"review_count\":number of reviews\n*  \"yelping_since\": date formatted like \"2009-12-19\"\n*  \"friends\":[\"an array of encrypted ids of friends\"]\n*  \"useful\":\"number of useful votes sent by the user\"\n*  \"funny\":\"number of funny votes sent by the user\"\n*  \"cool\":\"number of cool votes sent by the user\"\n*  \"fans\":\"number of fans the user has\"\n*  \"elite\":[\"an array of years the user was elite\"]\n*  \"average_stars\":floating point average like 4.31\n*  \"compliment_hot\":number of hot compliments received by the user\n*  \"compliment_more\":number of more compliments received by the user\n*  \"compliment_profile\": number of profile compliments received by the user\n*  \"compliment_cute\": number of cute compliments received by the user\n*  \"compliment_list\": number of list compliments received by the user\n*  \"compliment_note\": number of note compliments received by the user\n*  \"compliment_plain\": number of plain compliments received by the user\n*  \"compliment_cool\": number of cool compliments received by the user\n*  \"compliment_funny\": number of funny compliments received by the user\n*  \"compliment_writer\": number of writer compliments received by the user\n*  \"compliment_photos\": number of photo compliments received by the user\n*  \"type\":\"user\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about Yelp users in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nusers_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_user.json')\nusers_rdd.cache()\nprint users_rdd.count()\nprint users_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The third dataset we are going to load is information about business checkins reported by users on Yelp. Each checkin's information will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"checkin_info\":[\"an array of check ins with the format day-hour:number of check ins from hour to hour+1\"]\n*  \"business_id\":\"encrypted business id\"\n*  \"type\":\"checkin\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about business checkins reported by users on Yelp in an RDD\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\n# checkins_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_checkin.json')\n# checkins_rdd.cache()\n# print checkins_rdd.count()\n# print checkins_rdd.take(2)"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["The fourth dataset we are going to load is information about business reviews written by users on Yelp. Each review's data will be stored as a Python dictionary within an RDD. The dictionary consists of the following fields:\n\n*  \"review_id\":\"encrypted review id\"\n*  \"user_id\":\"encrypted user id\"\n*  \"business_id\":\"encrypted business id\"\n*  \"stars\":star rating rounded to half-stars\n*  \"date\":\"date formatted like 2009-12-19\"\n*  \"text\":\"review text\"\n*  \"useful\":number of useful votes received\n*  \"funny\":number of funny votes received\n*  \"cool\": number of cool review votes received\n*  \"type\": \"review\""],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# load the data about business reviews written by users on Yelp in an RDD, limited to businesses in Pittsburgh due to DataBricks computational limits\n# each RDD element is a Python dictionary parsed from JSON using json.loads()\n# if your chosen project does not need this data, please comment out the lines below\nreviews_rdd = get_rdd_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/yelp_academic_dataset_review_pittsburgh.json')\nreviews_rdd.cache()\nprint reviews_rdd.count()\nprint reviews_rdd.take(2)\n"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Finally, we will load two lists. The first list consists of male names, and the second list consists of female names. You can use these lists to predict the gender of Yelp users if you plan to do any gender-based analysis of users or their reviews."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# # helper function to load a list of names from a publicly accessible url\n# def get_names_from_url(url):\n#   response = urllib2.urlopen(url)\n#   str_contents = response.read().strip().split('\\n')\n#   result = str_contents[6:]\n#   return result\n\n# male_names = get_names_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/male.txt')\n# print('First five male names: ', male_names[:5])\n\n# female_names = get_names_from_url('https://www.andrew.cmu.edu/user/amaurya/docs/95869/female.txt')\n# print('First five female names: ', female_names[:5])\n\n\n# male_num = users_rdd.filter(lambda x: x['name'] in male_names).count()\n# female_num = users_rdd.filter(lambda x: x['name'] in female_names).count()\n# print male_num, female_num, male_num+female_num, users_rdd.count()"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["# ** Part 2: Introduction, Motivation, and Problem Definition **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nDescribe your chosen problem and why you think it is interesting from a business perspective. Also mention which of the four datasets you will use for the analysis. What metric(s) will you use to evaluate methods on your chosen task?"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Answer\nI am going to predict which type of a restaurant a user reviews most based on the restaurant types reviewed by his friends. It based on the assumption that people who are friends tend to review the similar restaurant types. \n\nI will use **user-based collaborative filtering algorithm** to predict and recommend a restaurant type (or top k) a user reviews most. **Precision** is the metric I will use to evaluate on the task. The datasets I need are **users, businesses, and reviews**. \n\nThe task is of great importance and business value because the model can contribute to the restaurant recommender system. It will at least bring about more reviews on restaurants on Yelp, which helps retain and attract users."],"metadata":{}},{"cell_type":"markdown","source":["# ** Part 3: Data Understanding and Preparation **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nDescribe your exploratory data analysis in this section. This is really important because it establishes that the datasets you are exploring is capable of answering your chosen project question. Make this section rich with visualization to give the reader a comprehensive understanding of the datasets you have chosen to use.\n\nBelow, you are provided helper code to install matplotlib's extra toolkit \"mpl_toolkits\" required for drawing maps. Also provided is an example map created using mpl_toolkits. You can refer to Matplotlib Basemap Toolkit documentation [here](https://matplotlib.org/basemap/)."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"code","source":["# %sh -e\n\n# # shell commands to install mpl_toolkits\n# sudo pip install matplotlib\n# cd /databricks\n# mkdir -p mpl_toolkit\n# cd mpl_toolkit\n\n# wget https://www.andrew.cmu.edu/user/amaurya/docs/95869/basemap-1.0.7.tar.gz\n# tar -xvf basemap-1.0.7.tar.gz\n\n# cd basemap-1.0.7/geos-3.3.3\n# export GEOS_DIR=/usr/local\n# ./configure --prefix=$GEOS_DIR\n# make; make install\n# cd ..\n# python setup.py install"],"metadata":{"collapsed":true,"deletable":true,"editable":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Data Understanding and Preparation (15%)\nWhat preliminary analyses have you performed on the data? What observations have you made? How did those observations help shape your approach?\nProvide the preliminary data analysis results and your observations.\nSpecify how the data will be transformed to the format required for machine learning."],"metadata":{}},{"cell_type":"markdown","source":["####(3a) Explore users\nFirst, let's convert users_rdd to DataFrame and see some statistic data about users.\n- total number of distinct users\n- max/min/avg number of user friends"],"metadata":{}},{"cell_type":"code","source":["def getFriendsList(item):\n  \"\"\" Returns a list of user_ids of the user's friends\n    Args:\n        user_id (string): user_id.\n\n    Returns:\n        list : user_ids of the user's friends.\n  \"\"\"\n  user_id = item['user_id']\n  friends = item['friends']\n  return map(lambda x: (user_id, x), friends)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql import Row\n# convert users_rdd to DataFrame\ndef get_user_friends_df(users_rdd, view):\n  \"\"\" Convert users_rdd to DataFrame\n    Args:\n        users_rdd (RDD): users.\n        view (str): table name.\n\n    Returns:\n        DataFrame : users with column of user_id and friends.\n  \"\"\"\n  user_friends = users_rdd.map(lambda x: getFriendsList(x)).flatMap(lambda x: x).map(lambda x: Row(user_id=x[0],friend_id=x[1]))\n  user_friends_DF = spark.createDataFrame(user_friends)\n  user_friends_DF.createOrReplaceTempView(view)\n  return user_friends_DF\n  \nuser_friends_DF = get_user_friends_df(users_rdd, 'user_friends')\nspark.sql(\"SELECT * FROM user_friends LIMIT 10\").show()\nuser_friends_DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["numUsers = user_friends_DF.select('user_id').distinct().count()\nfriends_len = users_rdd.map(lambda x: len(x['friends'])).cache()\nmaxNumFriends = friends_len.max()\nminNumFriends = friends_len.min()\navgNumFriends = friends_len.sum()/float(friends_len.count())\nprint 'Total number of distinct users {}'.format(numUsers)\nprint 'Maximum number of distinct users {}'.format(maxNumFriends)\nprint 'Minimum number of distinct users {}'.format(minNumFriends)\nprint 'Average number of distinct users {}'.format(avgNumFriends)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["####(3b) Explore businesses\nFor this task, we only consider restaurants. So we need a mapping between business id and business category for restaurants. We do the mapping first, then convert the mapping to a DataFrame, and see how many business types are there and the distribution of business types."],"metadata":{}},{"cell_type":"code","source":["# helper function to map business id and business categories under restaurant type\ndef mapBusinessIdAndCategories(business):\n  \"\"\"\n  Map business id and categories\n  \"\"\"\n  categories = business['categories']\n  business_id = business['business_id']\n  if 'Restaurants' in categories:\n    return map(lambda x: (business_id, x), categories)\n  return [None]\n  \n\n# convert business mapping to DataFrame\ndef get_business_mapping_df(businesses_rdd, view):\n  \"\"\" Convert businesses_rdd to DataFrame with columns of business_id and category\n    Args:\n        users_rdd (RDD): users.\n        view (str): table name.\n        \n    Returns:\n        DataFrame : business mapping with columns of business_id and category.\n  \"\"\"\n  # get (business_id, business_category) pairs\n  business_mapping = businesses_rdd.map(lambda x: mapBusinessIdAndCategories(x)).flatMap(lambda x: x).filter(lambda x: x != None and x[1] != 'Restaurants')\n  business_mapping_temp = business_mapping.map(lambda x: Row(business_id=x[0],category=x[1]))\n  business_mapping_DF = spark.createDataFrame(business_mapping_temp)\n  business_mapping_DF.createOrReplaceTempView(view)\n  return business_mapping_DF\n  \nbusiness_mapping_DF = get_business_mapping_df(businesses_rdd, 'business_mapping').cache()\nspark.sql(\"SELECT * FROM business_mapping LIMIT 10\").show()\nbusiness_mapping_DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["print 'Total restaurant businesses {}'.format(business_mapping_DF.select('business_id').distinct().count())\nprint 'Total restaurant business types {}'.format(business_mapping_DF.select('category').distinct().count())"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["####(3c) Explore reviews\nFor reviews, we will see how many reviews we have. And in fact, what we can get from reviews is (user_id, business_id) and what we really want is (user_id, business_category). So we need to map between business_id and business_category which we have done before. Here we create a user_category DataFrame to store (user_id, business_category) information."],"metadata":{}},{"cell_type":"code","source":["# convert business mapping to DataFrame\ndef get_reviews_df(reviews_rdd, view):\n  \"\"\" Convert businesses_rdd to DataFrame with columns of business_id and category\n    Args:\n        reviews_rdd (RDD): reviews.\n        view (str): table name.\n        \n    Returns:\n        DataFrame : reviews with columns of business_id and user_id.\n  \"\"\"\n  reviews_rdd_temp = reviews_rdd.map(lambda x: Row(user_id=x['user_id'],business_id=x['business_id']))\n  reviews_DF = spark.createDataFrame(reviews_rdd_temp)\n  reviews_DF.createOrReplaceTempView(view)\n  return reviews_DF\n\nreviews_DF = get_reviews_df(reviews_rdd, 'reviews')\nspark.sql(\"SELECT * FROM reviews LIMIT 10\").show()\nreviews_DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# map user_id, business_category, review count\ndef get_user_category_df(reviews_DF, business_mapping_DF, view):\n  \"\"\" Convert (user_id, business_id) to (user_id, business_category, review_count)\n    Args:\n        reviews_DF (DataFrame): reviews with columns of user_id and business_id.\n        business_mapping_DF (DataFrame): business mapping with columns of business_id and category.\n        view (str): table name.\n        \n    Returns:\n        DataFrame : DataFrame with columns of user_id, category, count.\n  \"\"\"\n  user_category_DF = reviews_DF.join(business_mapping_DF, reviews_DF.business_id == business_mapping_DF.business_id).select(reviews_DF.user_id, business_mapping_DF.category)\n  user_category_DF = user_category_DF.groupBy('user_id','category').count()\n  user_category_DF.createOrReplaceTempView(view)\n  return user_category_DF\n\ndef get_user_category_rdd(user_category_DF):\n  return user_category_DF.rdd.map(tuple)\n  \n\nuser_category_df = get_user_category_df(reviews_DF, business_mapping_DF, 'user_category')\nspark.sql(\"SELECT * FROM user_category LIMIT 10\").show()\nuser_category = get_user_category_rdd(user_category_df)\nuser_category.take(10)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["#### (3d) Explore top restaurant types"],"metadata":{}},{"cell_type":"code","source":["# business categories and reviews count\ndef mapBusinessCategoriesAndReviewCnt(business):\n  \"\"\"Map business categories and review count\n    Args:\n        business (json): business record\n        \n    Returns:\n        list: list of (business_category, review_count)\n  \"\"\"\n  categories = business['categories']\n  review_count = business['review_count']\n  if 'Restaurants' in categories:\n    return map(lambda x: (x, review_count), categories)\n  return [(None,0)]\n\n\nreviewCntByCategory = businesses_rdd.map(lambda x: mapBusinessCategoriesAndReviewCnt(x)).flatMap(lambda x: x).filter(lambda x: x[0]!='Restaurants' and x[0] != None).reduceByKey(lambda x,y:x+y).cache()\nreviewCntByCategory_desc = reviewCntByCategory.map(lambda x: (x[1],x[0])).sortByKey(False).map(lambda x: (x[1], x[0]))\nreviewCntByCategory_asc = reviewCntByCategory.map(lambda x: (x[1],x[0])).sortByKey().map(lambda x: (x[1], x[0]))\n\n\nnumCategories = reviewCntByCategory.count()\nprint 'Number of restaurant categories:{}'.format(numCategories)\nprint 'Top 10 category:{}'.format(reviewCntByCategory_desc.take(10))\nprint 'Last 10 category:{}'.format(reviewCntByCategory_asc.take(10))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# draw top 5 category distribution\nimport matplotlib.pyplot as plt\ntop_5 = reviewCntByCategory_desc.take(5)\ndata = [k[1] for k in top_5]\nlabels = [k[0] for k in top_5]\n\nfig, ax = plt.subplots()\nax.bar(range(len(data)), data)\nax.set_xticklabels(labels)\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["# ** Part 4: Methodology **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nIn this section, explain what method you have chosen to address the chosen problem. Are you going to be using regression, classification, clustering, topic modeling, collaborative filtering, or a combination of some of these? Describe why this method is suitable for answering your problem."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["### Answer\nThe **baseline model** is the restaurant type that has average reviews(all users reviews for type A/total number of types). \n\nI will use **user-based collaborative filtering algorithm** to predict and recommend a restaurant type (or top k) a user reviews most. **Precision** is the metric I will use to evaluate on the task. The model is as follow:\n- user-item matrix contains (user_id, category, review_count) pairs where review_count is the total reviews reviewed by the user for the restaurant category, the matrix can be considered as model.\n\nFor prediction,\n- get the friend list of the user\n- get (user_id, category, review_count) pairs where user_id is the user's friend's id\n- sum up all review_count by user's friends given a category and get all (user_id, category, review_count) pairs where user_id is the user, category is all types of restaurant reviewed by the user's friends, review_count is the total number of reviews reviewed by the user's friends given a category.\n- sort (user_id, category, review_count) in descending order by review_count and get top k categories\n\nUser-based collaborative filtering algorithm is quite straight-forward. It assumes friends will review same restaurant type. And the restuarnt type reviewed by his friends will also be the type the user reviews most.\n\nWe can also use classification algorithm like logistic regression as classifier where classes are the categories and features are the reviews by the user's friends. However there're too many categories (283 in total) which are also not real values. It may be hard to set a threshold to map probability with class. The probability generated by regression may not distinguable.\n\nI did not use MLlib for the solution because ALS is for learning latent factors but we make the assumption the similarity betweeen users is whether they are friends or not."],"metadata":{}},{"cell_type":"markdown","source":["#### (4a) Split dataset into training, validation, and test data and cache them for later use."],"metadata":{}},{"cell_type":"code","source":["# split training, validation, and test data\ndef split(rdd, weights, seed):\n  \"\"\" Split dataset rdd to training, validation, and test data\n    Args:\n        rdd (RDD): dataset\n        \n    Returns:\n        (RDD,RDD,RDD): training, validation, and test data\n  \"\"\"\n  return rdd.randomSplit(weights, seed)\n\n\nweights = [.8, .1, .1]\nseed = 42\n# split users_rdd\nusersTrainData, usersValData, usersTestData = users_rdd.randomSplit(weights, seed)\n# split businesses_rdd\nbusinessesTrainData, businessesValData, businessesTestData = businesses_rdd.randomSplit(weights, seed)\n# split reviews_rdd\nreviewsTrainData, reviewsValData, reviewsTestData = reviews_rdd.randomSplit(weights, seed)\n\n\n# model for train\nbusiness_mapping_train_df = get_business_mapping_df(businessesTrainData, 'business_mapping_train')\nreviews_train_df = get_reviews_df(reviewsTrainData, 'reviews_train')\nuser_category_train_df = get_user_category_df(reviews_train_df, business_mapping_train_df, 'user_category_train')\nuser_category_train_rdd = get_user_category_rdd(user_category_train_df)\n\n\n# df or rdd for validation or test\nuser_friends_val_df = get_user_friends_df(usersValData, 'user_friends_val')\nuser_friends_test_df = get_user_friends_df(usersTrainData, 'user_friends_test')\n\n\n# cache DataFrame\nuser_category_train_df.cache()\nuser_category_train_rdd.cache()\nuser_friends_val_df.cache()\n\n# cache tables\nsqlContext.cacheTable('user_category_train')\n\n# trainData.cache()\n# valData.cache()\n# testData.cache()\n# nTrain = trainData.count()\n# nVal = valData.count()\n# nTest = testData.count()\n\n# print 'Training: {}, Validation: {}, Test: {}, Total: {}'.format(nTrain, nVal, nTest, nTrain + nVal + nTest)\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["#### (4b) Helper Methods\nFollows are helper methods that returns the top k restaurant types the user reviews most."],"metadata":{}},{"cell_type":"code","source":["def getActualLabel(categories,k=1):\n  \"\"\" Returns top k restaurant types the user reviews most\n    Args:\n        user_id (string): user_id.\n\n    Returns:\n        list : list of restaurant type.\n  \"\"\"\n  if not categories: return None\n  return [x[0] for x in sorted(categories, key=lambda x: -x[1])[:k]]\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["def predict(train_rdd,k=1):\n  \"\"\" Returns top k restaurant types the user reviews most\n    Args:\n        user_id (string): user_id.\n\n    Returns:\n        list : list of restaurant type.\n  \"\"\"\n  user_category_groupByUser = train_rdd.map(lambda x: (x[0],(x[1],x[2]))).groupByKey().map(lambda x: (x[0], list(x[1])))\n  actualLabels = user_category_groupByUser.map(lambda x: (x[0],getActualLabel(x[1],k)))\n  return actualLabels\n"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### (4c) Create baseline model\nA very simple yet natural baseline model is one where we always make the same prediction independent of the given data point, using the average label in the training set as the constant prediction value."],"metadata":{}},{"cell_type":"code","source":["reviewTrainCntByCategory = businessesTrainData.map(lambda x: mapBusinessCategoriesAndReviewCnt(x)).flatMap(lambda x: x).filter(lambda x: x[0]!='Restaurants' and x[0] != None).reduceByKey(lambda x,y:x+y).cache()\n\navgReview = reviewTrainCntByCategory.map(lambda x: x[1]).mean()\navgLabel = reviewTrainCntByCategory.filter(lambda x: x[1]>=avgReview).take(1)[0]\n\nprint 'Average reviews {}'.format(avgReview)\nprint 'Label for average model {}'.format(avgLabel)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["predicted = avgLabel[0]\n# get distinct (user_id, '') for join purpose\nusersValDataUnique = usersValData.map(lambda x: (x['user_id'],'')).distinct().cache()\nbaseModel = usersValDataUnique.map(lambda x: (x[0], [predicted]))\n# get user_category(user_id, category, count) only with user_id in validation data\nremap_user_category_train_rdd = user_category_train_rdd.map(lambda x: (x[0],(x[1],x[2])))\nuser_category_val = usersValDataUnique.join(remap_user_category_train_rdd).map(lambda x: (x[0],x[1][1][0],x[1][1][1]))\nactualValLabels = predict(user_category_val).map(lambda x: (x[0],x[1][0])).cache()\nnVal = actualValLabels.count()\n"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["#### (4d) Create User-based Collaborative Filtering Model"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import sum\ndef user_category_cnt(user_friends_df, user_category_df):\n  user_category_by_friends = user_friends_df.join(user_category_df, user_friends_df.friend_id==user_category_df.user_id).groupBy(user_friends_df.user_id, 'category').agg(sum('count'))\n  return user_category_by_friends.rdd.map(tuple)\n\n\n# cf model\ncf_user_category = user_category_cnt(user_friends_val_df, user_category_train_df)\ncf_user_category.cache()\ncfModel = predict(cf_user_category)\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["# ** Part 5: Evaluation and Results **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nIn this section, describe all experiemntal parameters such as those used for grid search on hyperparameters. Include results of the chosen methods on your task. How does the metric of interest vary with changes in important method hyperparameters such as regularization, number of iterations, etc.?"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["I use **precision** as the metric. We get top k recommended restaurant types and see if any of the top k types hit the acutal restaurant type the user reviews most. Grid search is used to get the best k.\n\nThe model improved \n\n**Recall** is not very useful in this case since the actual label is one restaurant type."],"metadata":{}},{"cell_type":"code","source":["def getPrecision(predicted, actual, nVal):\n  # resulst would be (user_id, (recommended_list, actual))\n  joinedRdd = predicted.join(actual)\n  hit = joinedRdd.filter(lambda x: x[1][1] in x[1][0]).count()\n  return hit/float(nVal)\n\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# grid-search\ncf_user_category = user_category_cnt(user_friends_val_df, user_category_train_df).cache()\nbestK = 1\nbestPrecision = 0\nbestModel = None\nfor topk in [1,3,5,7]:\n  model = predict(cf_user_category,topk)\n  precision = getPrecision(model, actualValLabels, nVal)\n  print ('Current k={}, precision={}').format(topk, precision)\n  if precision > bestPrecision:\n    bestPrecision = precision\n    bestK = topk\n    bestModel = model\n    \n    \nprecisionBase = getPrecision(baseModel, actualValLabels, nVal)\n\nprint ('Best k {}').format(bestK)\nprint ('Validation Precision:\\n\\tBaseline = {}\\n\\tCFGrid = {}').format(precisionBase,bestPrecision)\nprint ('Improved {}%').format((bestPrecision-precisionBase)/precisionBase)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["# ** Part 6: Conclusions **"],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["Please write your answer here. Add additional IPython code/markup cells as needed. Please the grading rubric at the top of this notebook to understand expectations from this section.\n\nThis should be a short final section stating whether the methods you explored on Yelp dataset were able to satisfactorily solve the problem you set out to solve. Discuss any business implications of the performance metrics you obtained such as accuracy, RMSE, runtime, etc. Finally, state if you think this implementation is ready for production-deployment or if there are kinks that need to be worked through before it is usable."],"metadata":{"deletable":true,"editable":true}},{"cell_type":"markdown","source":["#### Answer\nAs shown above, the precision improved around 185% compared to baseline model, which is a huge improvement. Actually there's no doubt that increasing k will increase the precision. However, we cannot recommend as many items as we have to the users. Top 3-5 may be a better choice. However, it's not ready for production-deployment because the prediction is a little bit slow. Further optimization is required. Also, the model is cached offline. What if there's new users, or the user makes new friends? Current model cannot handle it."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.10","nbconvert_exporter":"python","file_extension":".py"},"name":"project_yelp_dataset","notebookId":4190433212045478},"nbformat":4,"nbformat_minor":0}
